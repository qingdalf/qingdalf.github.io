{"meta":{"title":"倚楼听雨","subtitle":"welcome to my blog.","description":"倚楼听风雨，淡看江湖路。","author":"qingdalf","url":"https://scaven.site","root":"/"},"pages":[{"title":"分类","date":"2021-04-20T02:37:39.000Z","updated":"2021-04-20T02:42:41.038Z","comments":false,"path":"categories/index.html","permalink":"https://scaven.site/categories/index.html","excerpt":"","text":"HomeLinuxpythonkubernetsDevOpsAboutMedocker"},{"title":"tags","date":"2021-04-20T02:32:02.000Z","updated":"2021-04-20T02:40:52.053Z","comments":false,"path":"tags/index.html","permalink":"https://scaven.site/tags/index.html","excerpt":"","text":"welcomeLinuxpythonkubernetsDevOpsAboutMe"},{"title":"关于","date":"2021-04-18T14:57:22.000Z","updated":"2021-04-19T02:41:56.107Z","comments":true,"path":"bak/about/index.html","permalink":"https://scaven.site/bak/about/index.html","excerpt":"","text":"贺新郎·甚矣吾衰矣 【作者】辛弃疾 【朝代】宋 邑中园亭，仆皆为赋此词。一日，独坐停云，水声山色，竞来相娱。意溪山欲援例者，遂作数语，庶几仿佛渊明思亲友之意云。 甚矣吾衰矣。怅平生、交游零落，只今余几！白发空垂三千丈，一笑人间万事。问何物、能令公喜？我见青山多妩媚，料青山见我应如是。情与貌，略相似。 一尊搔首东窗里。想渊明、停云诗就，此时风味。江左沉酣求名者，岂识浊醪妙理。回首叫、云飞风起。不恨古人吾不见，恨古人、不见吾狂耳。知我者，二三子。"},{"title":"归档","date":"2021-04-18T14:54:26.000Z","updated":"2021-04-18T14:55:18.321Z","comments":false,"path":"bak/archives/index.html","permalink":"https://scaven.site/bak/archives/index.html","excerpt":"","text":""},{"title":"分类","date":"2021-04-18T14:56:25.000Z","updated":"2021-04-18T14:57:10.386Z","comments":false,"path":"bak/categories/index.html","permalink":"https://scaven.site/bak/categories/index.html","excerpt":"","text":""},{"title":"日历","date":"2021-04-18T14:55:33.000Z","updated":"2021-04-18T14:56:12.693Z","comments":false,"path":"bak/schedule/index.html","permalink":"https://scaven.site/bak/schedule/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-04-18T14:07:05.000Z","updated":"2021-04-20T02:27:33.658Z","comments":false,"path":"bak/tags/index.html","permalink":"https://scaven.site/bak/tags/index.html","excerpt":"","text":"welcomelinuxpythonkubernetsDevOpsAboutMe"}],"posts":[{"title":"k8s-service","slug":"k8s-service","date":"2021-04-20T16:00:06.000Z","updated":"2021-04-20T16:02:27.791Z","comments":true,"path":"2021-04/21-k8s-service/","link":"","permalink":"https://scaven.site/2021-04/21-k8s-service/","excerpt":"","text":"第七章 Service对外暴露应用1. Service是什么Q1:如何让前端连接后端？ ​ 物理服务器/虚拟机都是通过IP地址相互访问的，IP地址是固定的，那么为啥kubernets不能通过ip访问呢 Pod的IP地址是随机分配的 Pod的IP随着Pod的启动而变化 ​ 容器的IP地址是临时的(随着容器的启动随机进行分配)，且多个pod有多个IP地址，因此我们不能通过IP地址进行连接，我们需要使用一个统一的访问入口，这个统一的访问入口就等同于一个负载均衡器，这时Service就排上了用场。 Q2：后端程序多个Pod如何同时提供服务？ ​ 通过统一的访问入口，让Service统一将请求分发(负载均衡)到后端程序提供服务，并且Service拥有动态感知Pod的能力，Pod的IP发生变化或者有新的Pod加入，Service能自动将流量分发到这些变化和新加入的Pod上 总结： ​ 前端用户(user)–连接LB的IP地址，LB将用户流量分发到后端pod，并随时感知pod的状态变化，自动更新负载策略(如，pod故障就不转发流量到故障pod上；又新加入的pod，自动将podip加入负载策略中) 2. Service存在的意义Service引入主要是解决了Pod的动态变化，提供统一的访问入口： 防止Pod失联，准备找到提供同一个服务器的Pod(服务发现) 定义一组Pod的访问策略(负载均衡) Pod与Service的关系： Service通过标签关联Pod Service使用iptables或者ipvs为一组Pod提供负载均衡能力 123456789101112131415# kubectl get pod --show-labels # 通过--show-labels查看标签NAME READY STATUS RESTARTS AGE LABELSjava-demo-56d54df448-7mhg8 1/1 Running 0 11h app=java-demo,pod-template-hash=56d54df448java-demo-56d54df448-dx767 1/1 Running 0 11h app=java-demo,pod-template-hash=56d54df448java-demo-56d54df448-flh48 1/1 Running 0 11h app=java-demo,pod-template-hash=56d54df448nginx-6799fc88d8-825ps 1/1 Running 0 34h app=nginx,pod-template-hash=6799fc88d8web-86cd4d65b9-554kq 1/1 Running 0 11h app=web,pod-template-hash=86cd4d65b9web-86cd4d65b9-65nkq 1/1 Running 0 11h app=web,pod-template-hash=86cd4d65b9web-86cd4d65b9-7zml9 1/1 Running 0 11h app=web,pod-template-hash=86cd4d65b9# kubectl get pod -l app=web # 通过-l过滤标签NAME READY STATUS RESTARTS AGEweb-86cd4d65b9-554kq 1/1 Running 0 11hweb-86cd4d65b9-65nkq 1/1 Running 0 11hweb-86cd4d65b9-7zml9 1/1 Running 0 11h 3. Service的定义与创建3.1 命令行创建123# 此处我们不做创建，只做检查# kubectl expose deployment web --port=80 --target-port=80 --dry-run=clientservice/web exposed (dry run) 3.2 yaml文件创建12# 如果不会写yaml，我们仍然可以通过--dry-run=client -o yaml将yaml文件导出# kubectl expose deployment web --port=80 --target-port=80 --dry-run=client -o yaml &gt; web_service.yaml 4.Service的三种类型4.1 ClusterIP​ 默认，分配一个稳定的IP地址，即VIP，只能在集群内部访问。用户不能直接访问，只是解决了如何在集群内部访问，即node和pod，pod和pod之间的访问。 1234567891011121314151617# cat web_service.yaml # 通过是上面的命令生成的yaml文件。默认类型为clusterIPapiVersion: v1kind: Servicemetadata: creationTimestamp: null # 删除创建时间戳 labels: # svc的标签 app: web name: web # svc的名称spec: ports: - port: 80 # Service暴露的端口 protocol: TCP targetPort: 80 # 容器使用的端口，即将容器的80端口与Service的80端口做关联 selector: # 关联后端的Pod的标签，即通过这个标签去找到对应的pod，跟资源(如pod,deploy)类型中的metadata.labels一致 app: webstatus: # 删除该字段 loadBalancer: &#123;&#125; 4.2 NodePort​ 在每个节点上启用一个端口来暴露服务，可以在集群外部访问，但也会创建一个稳定的ClusterIP，端口在30000-32767(端口可指定，默认自动分配)之间，访问地址为&lt;任意NodeIP&gt;:，这样会在每台Node上监听端口接收用户流量，在实际情况下，对用户暴露的只会有一个IP和端口，那这么多台用哪台让用户访问呢？一般的做法是加一个公网负载均衡器，让项目提供统一的访问入口 企业常见用法： 12345678910111213141516171819# cat web_service.yaml # 指定端口和类型apiVersion: v1kind: Servicemetadata: creationTimestamp: null # 删除创建时间戳 labels: app: web name: web spec: type: NodePort # 指定类型为NodePort,可在集群外访问 ports: - port: 80 protocol: TCP targetPort: 80 nodePort: 30023 # 如果想指定端口，可添加该字段，不指定端口则自动分配，范围在30000-32767 selector: app: webstatus: # 删除该字段 loadBalancer: &#123;&#125; 4.3 LoadBalancer (LB)​ 与NodePort类似，适用于公有云，在每个节点启用一个端口来暴露服务，除此之外，k8s会请求底层云平台(如阿里云，aws等)上的负载均衡器，自动将每个Node(&lt;任意NodeIP&gt;:)作为后端添加进去，可查阅云厂商相关文档 5. Service代理模式 iptables(默认规则) 灵活，功能强大 规则遍历匹配和更新，呈线性时延，不适用于大集群(建议service小于100，pod副本数跟service比例在1:3-5可使用iptables) 查看iptables规则：iptables-save |grep 查看规则的方法： 1234567891011121314151617# 1. 通过kube-proxy的配置文件# 2. 通过kube-proxy的日志，我们是通过容器启动的，因此就通过该方法查看# kubectl get pods -n kube-system | grep proxykube-proxy-bzhts 1/1 Running 1 11dkube-proxy-fd5q2 1/1 Running 1 11dkube-proxy-p6hmt 1/1 Running 1 11d# kubectl logs kube-proxy-p6hmt -n kube-system # 查看任意一个proxy Pod的日志I0420 06:35:30.109968 1 node.go:172] Successfully retrieved node IP: 10.138.3.61I0420 06:35:30.110014 1 server_others.go:142] kube-proxy node IP is an IPv4 address (10.138.3.61), assume IPv4 operationW0420 06:35:30.134968 1 server_others.go:578] Unknown proxy mode &quot;&quot;, assuming iptables proxy# 此处的&quot;proxy mode &quot;&quot;, assuming iptables proxy&quot;表示，如果模式为空，则使用iptables规则I0420 06:35:30.135048 1 server_others.go:185] Using iptables Proxier.I0420 06:35:30.135559 1 server.go:650] Version: v1.20.0I0420 06:35:30.135919 1 conntrack.go:100] Set sysctl &#x27;net/netfilter/nf_conntrack_max&#x27; to 131072···· ipvs 工作在内核，有更好的性能，适合于大集群，建议使用 调度算法丰富：rr，wrr，lc，wlc，ip hash··· 查看ipvs规则：ipvsadm -L -n 5.1 iptables12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# 获取容器IP，iptables规则一定会将流量转发到容器IP# kubectl get pods -o wide | grep nginx # 记住容器IP，iptables规则一定会将流量转发到容器IPnginx-6799fc88d8-825ps 1/1 Running 1 10d 10.244.154.198 k8s-node-01 &lt;none&gt; &lt;none&gt;# 获取svc IP，svc IP一定会去关联后端的容器IP，才能让用户访问到应用信息# kubectl get svc -o wide | grep nginx nginx NodePort 10.103.247.121 &lt;none&gt; 80:32561/TCP 11d app=nginx# 端点信息# kubectl get ep | grep nginxnginx 10.244.154.198:80 11d# ss -lntp | grep proxyLISTEN 0 128 127.0.0.1:10249 *:* users:((&quot;kube-proxy&quot;,pid=2994,fd=18))LISTEN 0 128 *:32561 *:* users:((&quot;kube-proxy&quot;,pid=2994,fd=13))LISTEN 0 128 *:30001 *:* users:((&quot;kube-proxy&quot;,pid=2994,fd=10))LISTEN 0 128 [::]:10256 [::]:* users:((&quot;kube-proxy&quot;,pid=2994,fd=19))# iptables规则# iptables-save | grep nginx # 此处的nginx是service的名称## 1.访问nodePort，端口是32561时，将流量转发到KUBE-SVC-2CMXP7HKUVJN7L6M链-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/nginx&quot; -m tcp --dport 32561 -j KUBE-MARK-MASQ-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/nginx&quot; -m tcp --dport 32561 -j KUBE-SVC-2CMXP7HKUVJN7L6M ## 3.将来自KUBE-SEP-FIC36SCPQQINK7HE链的规则，通过DNAT(目标地址转换)转发到10.244.154.198:80(容器IP和端口)容器中## 如果有多个容器，会添加多条相同的规则和负载规则策略，只是--to-destination的IP地址不同-A KUBE-SEP-FIC36SCPQQINK7HE -s 10.244.154.198/32 -m comment --comment &quot;default/nginx&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-FIC36SCPQQINK7HE -p tcp -m comment --comment &quot;default/nginx&quot; -m tcp -j DNAT --to-destination 10.244.154.198:80-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.103.247.121/32 -p tcp -m comment --comment &quot;default/nginx cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ## 1.访问clusterIP的转发，目标地址是10.103.247.121/32(svc地址)，目标端口是80 ，协议是tcp的流量，转发到KUBE-SVC-2CMXP7HKUVJN7L6M链中-A KUBE-SERVICES -d 10.103.247.121/32 -p tcp -m comment --comment &quot;default/nginx cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-2CMXP7HKUVJN7L6M## 2.从nodeport和clusterIP两种类型来的流量，都来匹配这条规则## 含义是：把所有从KUBE-SVC-2CMXP7HKUVJN7L6M链来的规则都交给KUBE-SEP-FIC36SCPQQINK7HE链去处理-A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment &quot;default/nginx&quot; -j KUBE-SEP-FIC36SCPQQINK7HE# 我们将集群nginx容器扩容到3个pod# kubectl scale deployment nginx --replicas=3 deployment.apps/nginx scaled# 等待创建完所有pod# kubectl get ep | grep nginx # 这里是所有nginx容器的IP地址和端口nginx 10.244.154.198:80,10.244.44.209:80,10.244.44.210:80 11d# 我们再次来查看iptables规则# iptables-save | grep nginx## 1. 从nodeport来的流量-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/nginx&quot; -m tcp --dport 32561 -j KUBE-MARK-MASQ-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/nginx&quot; -m tcp --dport 32561 -j KUBE-SVC-2CMXP7HKUVJN7L6M# 3.上文中存在的转发规则-A KUBE-SEP-FIC36SCPQQINK7HE -s 10.244.154.198/32 -m comment --comment &quot;default/nginx&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-FIC36SCPQQINK7HE -p tcp -m comment --comment &quot;default/nginx&quot; -m tcp -j DNAT --to-destination 10.244.154.198:80## 3.多出来的第一个规则，转发到第二个pod-A KUBE-SEP-FU7B2O7TL4BNEJXI -s 10.244.44.210/32 -m comment --comment &quot;default/nginx&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-FU7B2O7TL4BNEJXI -p tcp -m comment --comment &quot;default/nginx&quot; -m tcp -j DNAT --to-destination 10.244.44.210:80# 3.多出来的第二个规则，转发到第三个pod-A KUBE-SEP-HJU6P624NH7ZFLEO -s 10.244.44.209/32 -m comment --comment &quot;default/nginx&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-HJU6P624NH7ZFLEO -p tcp -m comment --comment &quot;default/nginx&quot; -m tcp -j DNAT --to-destination 10.244.44.209:80# 1.从clusterIP来的流量-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.103.247.121/32 -p tcp -m comment --comment &quot;default/nginx cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ-A KUBE-SERVICES -d 10.103.247.121/32 -p tcp -m comment --comment &quot;default/nginx cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-2CMXP7HKUVJN7L6M## 下面的三条策略，从上倒下匹配--Iptables使用概率来保障pod负载均衡分配# 多出来的转发策略---A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment &quot;default/nginx&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-FIC36SCPQQINK7HE# 多出来的转发策略---A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment &quot;default/nginx&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-HJU6P624NH7ZFLEO# 默认的匹配(转发)规则---A KUBE-SVC-2CMXP7HKUVJN7L6M -m comment --comment &quot;default/nginx&quot; -j KUBE-SEP-FU7B2O7TL4BNEJXI 根据上面的iptables规则可以知道，只是入口不一样(nodeport或clusterip)，入口后的规则都是一样的，因此一般情况下数据包准发流程如下： 通过访问clusterIP来访问pod(集群内)，即user–&gt;clusterIP–&gt;PodIP(多个IP) 通过访问nodeIP来访问pod(集群外)，即user–&gt;NodePort(多个集群IP)–&gt;PodIP(多个IP) 5.2 ipvs​ 通过上文，我们已经知道了默认模式是iptables，如果我们想使用ipvs规则，需要进行模式的修改。 修改方式： 二进制部署，通过配置文件kube-proxy-config.yml修改，修改imode，然后重启kube-proxy服务 容器部署，通过kubeadm的方式修改，此处采用该方式 12# kubectl edit configmap kube-proxy -n kube-systemconfigmap/kube-proxy edited 如下图： 重置容器： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# k8s中没有容器Pod的概念，因此此处直接删除pod，让其重建pod# kubectl get pods -n kube-system -o wide | grep proxy kube-proxy-bzhts 1/1 Running 1 11d 10.138.3.62 k8s-node-01 &lt;none&gt; &lt;none&gt;kube-proxy-fd5q2 1/1 Running 1 11d 10.138.3.63 k8s-node-02 &lt;none&gt; &lt;none&gt;kube-proxy-p6hmt 1/1 Running 1 11d 10.138.3.61 k8s-master &lt;none&gt; &lt;none&gt;# 先删除其中一个(k8s-node-01)，然后验证合法后，再进行其他两台的操作# kubectl delete pod kube-proxy-bzhts -n kube-systempod &quot;kube-proxy-bzhts&quot; deleted# kubectl get pods -n kube-system -o wide | grep proxy kube-proxy-fd5q2 1/1 Running 1 11d 10.138.3.63 k8s-node-02 &lt;none&gt; &lt;none&gt;kube-proxy-lndks 1/1 Running 0 8s 10.138.3.62 k8s-node-01 &lt;none&gt; &lt;none&gt;kube-proxy-p6hmt 1/1 Running 1 11d 10.138.3.61 k8s-master &lt;none&gt; &lt;none&gt;# kubectl get svc -o wide | grep nginxnginx NodePort 10.103.247.121 &lt;none&gt; 80:32561/TCP 11d app=nginx# 验证(在k8s-node-01验证)：安装ipvs，并查看规则# yum -y install ipvsadm# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.96.0.1:443 rr # 默认规则就是rr -&gt; 10.138.3.61:6443 Masq 1 0 0 TCP 10.96.0.10:53 rr -&gt; 10.244.235.196:53 Masq 1 0 0 -&gt; 10.244.235.197:53 Masq 1 0 0 TCP 10.96.0.10:9153 rr -&gt; 10.244.235.196:9153 Masq 1 0 0 -&gt; 10.244.235.197:9153 Masq 1 0 0 TCP 10.99.141.98:8000 rr -&gt; 10.244.44.208:8000 Masq 1 0 0 TCP 10.100.90.183:443 rr -&gt; 10.244.154.197:8443 Masq 1 0 0 TCP 10.103.247.121:80 rr # svc的IP地址：10.103.247.121，通过svc地址(clusterIP)访问的流量转发 -&gt; 10.244.44.209:80 Masq 1 0 0 -&gt; 10.244.44.210:80 Masq 1 0 0 -&gt; 10.244.154.198:80 Masq 1 0 0 TCP 10.138.3.62:30001 rr -&gt; 10.244.154.197:8443 Masq 1 0 0 TCP 10.138.3.62:32561 rr # 宿主机IP地址：10.138.3.62，通过nodeport访问的流量转发 -&gt; 10.244.44.209:80 Masq 1 0 0 -&gt; 10.244.44.210:80 Masq 1 0 0 -&gt; 10.244.154.198:80 Masq 1 0 0 TCP 10.244.154.192:30001 rr -&gt; 10.244.154.197:8443 Masq 1 0 0 TCP 10.244.154.192:32561 rr # 通过网卡tunl0@NONE访问，网卡信息看下文 -&gt; 10.244.44.209:80 Masq 1 0 0 -&gt; 10.244.44.210:80 Masq 1 0 0 -&gt; 10.244.154.198:80 Masq 1 0 0 TCP 127.0.0.1:30001 rr -&gt; 10.244.154.197:8443 Masq 1 0 0 TCP 127.0.0.1:32561 rr # 通过lo:32561访问nginx容器(NodePort) -&gt; 10.244.44.209:80 Masq 1 0 0 -&gt; 10.244.44.210:80 Masq 1 0 0 -&gt; 10.244.154.198:80 Masq 1 0 0 TCP 172.17.0.1:30001 rr -&gt; 10.244.154.197:8443 Masq 1 0 0 TCP 172.17.0.1:32561 rr # 172.17.0.1 docker0的IP地址 -&gt; 10.244.44.209:80 Masq 1 0 0 -&gt; 10.244.44.210:80 Masq 1 0 0 -&gt; 10.244.154.198:80 Masq 1 0 0 UDP 10.96.0.10:53 rr -&gt; 10.244.235.196:53 Masq 1 0 0 -&gt; 10.244.235.197:53 Masq 1 0 0 # ip a # 网卡信息发生了变化1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:f8:4c:a3 brd ff:ff:ff:ff:ff:ff inet 10.138.3.62/24 brd 10.138.3.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::edf4:6c5f:a090:110/64 scope link noprefixroute valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:b9:85:b5:e8 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever4: tunl0@NONE: &lt;NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.0 # 不知道干啥的？？？？ inet 10.244.154.192/32 scope global tunl0 valid_lft forever preferred_lft forever5: cali76d2c4cb462@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::ecee:eeff:feee:eeee/64 scope link valid_lft forever preferred_lft forever6: cali2941b9a4026@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UP group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::ecee:eeff:feee:eeee/64 scope link valid_lft forever preferred_lft forever7: dummy0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 02:a3:b5:68:00:72 brd ff:ff:ff:ff:ff:ff8: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default # 新出现的网卡 link/ether ae:50:d0:2b:72:2b brd ff:ff:ff:ff:ff:ff inet 10.96.0.1/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.103.247.121/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.96.0.10/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.99.141.98/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.100.90.183/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever # 删除其他两个proxy pod# kubectl get pods -o wide -n kube-system |grep proxy\\kube-proxy-fd5q2 1/1 Running 1 11d 10.138.3.63 k8s-node-02 &lt;none&gt; &lt;none&gt;kube-proxy-lndks 1/1 Running 0 39m 10.138.3.62 k8s-node-01 &lt;none&gt; &lt;none&gt;kube-proxy-p6hmt 1/1 Running 1 11d 10.138.3.61 k8s-master &lt;none&gt; &lt;none&gt; # kubectl delete pod kube-proxy-fd5q2 -n kube-systempod &quot;kube-proxy-fd5q2&quot; deleted# kubectl delete pod kube-proxy-p6hmt -n kube-systempod &quot;kube-proxy-p6hmt&quot; deleted# 完成如下：# kubectl get pods -o wide -n kube-system |grep proxykube-proxy-7xhzn 1/1 Running 0 10s 10.138.3.61 k8s-master &lt;none&gt; &lt;none&gt;kube-proxy-hr7ht 1/1 Running 0 91s 10.138.3.63 k8s-node-02 &lt;none&gt; &lt;none&gt;kube-proxy-lndks 1/1 Running 0 41m 10.138.3.62 k8s-node-01 &lt;none&gt; &lt;none&gt; 5.3 工作模式 6. Service DNS名称​ CoreDNS是一个DNS服务器，k8s默认采用，以Pod部署在集群中，CoreDNS服务监视K8S API，为每个Servicd创建DNS记录用于域名解析。 ​ ClusterIP A记录格式：..svc.cluster.local，如果查询不到要么是svc不存在，要么没有指定命名空间。 1234567891011121314151617181920212223242526272829303132333435363738394041424344# kubectl get svc| grep nginx nginx NodePort 10.103.247.121 &lt;none&gt; 80:32561/TCP 11d# kubectl get svc -o wide -n kube-system # 名叫kube-dns的svc，这个就是dns的IPNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 11d k8s-app=kube-dns# kubectl run -it --image=busybox:1.28.4 -- sh/ # nslookup nginx # 查看nginx的解析信息Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: nginxAddress 1: 10.103.247.121 nginx.default.svc.cluster.local/ # cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.localnameserver 10.96.0.10options ndots:5## 在其他node上删除nginx这个svc,再次查询nginx的解析信息# kubectl delete svc nginxservice &quot;nginx&quot; deleted# 在次查询Nginx的解析信息/ # nslookup nginxServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localnslookup: can&#x27;t resolve &#x27;nginx&#x27; # 查询失败## 在其他节点上，新建nginx的svc后，在查看解析信息# kubectl expose deployment nginx --target-port=80 --port=80 --type=NodePortservice/nginx exposed# kubectl get svc |grep nginxnginx NodePort 10.99.5.83 &lt;none&gt; 80:31031/TCP 20s # nginx的IP地址已经发生了变化# 再一次解析/ # nslookup nginxServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: nginxAddress 1: 10.99.5.83 nginx.default.svc.cluster.local # 解析成功","categories":[{"name":"docker","slug":"docker","permalink":"https://scaven.site/categories/docker/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"https://scaven.site/tags/kubernets/"}]},{"title":"aboutme","slug":"aboutme","date":"2021-04-20T04:14:50.000Z","updated":"2021-04-20T04:16:26.643Z","comments":true,"path":"2021-04/20-aboutme/","link":"","permalink":"https://scaven.site/2021-04/20-aboutme/","excerpt":"","text":"贺新郎·甚矣吾衰矣 【作者】辛弃疾 【朝代】宋 邑中园亭，仆皆为赋此词。一日，独坐停云，水声山色，竞来相娱。意溪山欲援例者，遂作数语，庶几仿佛渊明思亲友之意云。 甚矣吾衰矣。怅平生、交游零落，只今余几！白发空垂三千丈，一笑人间万事。问何物、能令公喜？我见青山多妩媚，料青山见我应如是。情与貌，略相似。 一尊搔首东窗里。想渊明、停云诗就，此时风味。江左沉酣求名者，岂识浊醪妙理。回首叫、云飞风起。不恨古人吾不见，恨古人、不见吾狂耳。知我者，二三子。","categories":[],"tags":[{"name":"AboutMe","slug":"AboutMe","permalink":"https://scaven.site/tags/AboutMe/"}]},{"title":"docker","slug":"docker","date":"2021-04-20T04:02:58.000Z","updated":"2021-04-20T04:10:03.900Z","comments":true,"path":"2021-04/20-docker/","link":"","permalink":"https://scaven.site/2021-04/20-docker/","excerpt":"","text":"","categories":[],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"https://scaven.site/tags/kubernets/"}]},{"title":"welcome","slug":"welcome","date":"2021-04-20T02:11:08.000Z","updated":"2021-04-20T02:42:04.832Z","comments":true,"path":"2021-04/20-welcome/","link":"","permalink":"https://scaven.site/2021-04/20-welcome/","excerpt":"","text":"welcome to my blog!","categories":[{"name":"Home","slug":"Home","permalink":"https://scaven.site/categories/Home/"}],"tags":[{"name":"welcome","slug":"welcome","permalink":"https://scaven.site/tags/welcome/"}]},{"title":"docker-registory","slug":"docker-registory","date":"2021-04-19T01:09:44.000Z","updated":"2021-04-20T02:42:46.022Z","comments":true,"path":"2021-04/19-docker-registory/","link":"","permalink":"https://scaven.site/2021-04/19-docker-registory/","excerpt":"","text":"第一章 docker私有仓库–harbor的搭建1. harbor是什么1.1 harbor简介​ Harbor是由VMware公司开源的一个用于存储和分发Docker镜像的企业级docker镜像仓库，默认使用https访问协议。通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。 云本机注册表：Harbour 支持容器映像和Helm图表，可用作云本机环境（如容器运行时和业务流程平台）的注册表。 基于角色的访问控制：用户和存储库通过“项目”进行组织，用户可以对项目下的图像拥有不同的权限。 基于策略的映像复制：可以基于具有多个过滤器（存储库，标记和标签）的策略在多个注册表实例之间复制（同步）映像。如果遇到任何错误，Harbor将自动重试进行复制。非常适合负载平衡，高可用性，多数据中心，混合和多云场景。 漏洞扫描：Harbor定期扫描图像并警告用户漏洞。 LDAP / AD支持：Harbor与现有企业LDAP / AD集成以进行用户身份验证和管理，并支持将LDAP组导入Harbor并为其分配适当的项目角色。 图像删除和垃圾收集：可以删除图像，并可以回收它们的空间。 公证：可以确保图像的真实性。 图形用户门户：用户可以轻松浏览，搜索存储库和管理项目。 审计：跟踪存储库的所有操作。 RESTful API：适用于大多数管理操作的RESTful API，易于与外部系统集成。 易于部署：提供在线和离线安装程序。 官方文档：https://goharbor.io/docs/ github用户手册：https://github.com/goharbor/harbor/blob/master/docs/user_guide.md 官网：https://harbor.com/ 1.2 harbor组件： 组件 功能 harbor-adminserver 配置管理中心 harbor-db Mysql数据库 harbor-jobservice 负责镜像复制 harbor-log 记录操作日志 harbor-ui Web管理页面和API nginx 前端代理，负责前端页面和镜像上传/下载转发 redis 会话 registry 镜像存储 2 harbor下载与安装2.1 harbor下载服务器配置要求：2C/4G/40G，推荐4C/8G/160G 下载地址：https://github.com/goharbor/harbor/releases 2.1.1 安装方式 离线安装：下载harbor软件包到本地进行安装，软件包较大，下载速度慢 在线安装： OVA安装程序：当用户具有vCenter环境时，使用此安装程序，在部署OVA后启动Harbor。 2.2 harbor安装2.2.1 安装docker1234567891011121314151617181920212223# 如果已经安装了docker，可以使用下面的命令卸载# yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine # 安装必要的依赖包# yum install -y device-mapper-persistent-data lvm2# 添加docker仓库，并修改成国内清华镜像源# wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo # sed -i &#x27;s#download.docker.com#mirrors.tuna.tsinghua.edu.cn/docker-ce#&#x27; /etc/yum.repos.d/docker-ce.repo# yum clean all &amp;&amp; yum repolist# 安装docker# yum -y install docker-ce# 添加镜像加速器# mkdir /etc/docker# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;http://hub-mirror.c.163.com&quot;]&#125;# 启动并添加到开机自启动# systemctl start docker &amp;&amp; systemctl enable docker 2.2.2 安装docker-compose​ harbor使用docker-compose进行编排，因此在安装harbor之前必须先安装docker-compose。 ​ 下载地址：https://github.com/docker/compose/releases 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# ls docker-compose-Linux-x86_64docker-compose-Linux-x86_64# 添加执行权限并移动到环境变量路径中# chmod u+x docker-compose-Linux-x86_64# mv docker-compose-Linux-x86_64 /usr/bin/docker-compose # 验证 # docker-compose --versiondocker-compose version 1.28.6, build 5db8d86f# docker-compose简单用法# docker-compose --helpDefine and run multi-container applications with Docker.Usage: docker-compose [-f &lt;arg&gt;...] [--profile &lt;name&gt;...] [options] [--] [COMMAND] [ARGS...] docker-compose -h|--helpOptions: -f, --file FILE Specify an alternate compose file (default: docker-compose.yml) -p, --project-name NAME Specify an alternate project name (default: directory name) --profile NAME Specify a profile to enable -c, --context NAME Specify a context name --verbose Show more output --log-level LEVEL Set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL) --ansi (never|always|auto) Control when to print ANSI control characters --no-ansi Do not print ANSI control characters (DEPRECATED) -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don&#x27;t check the daemon&#x27;s hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory (default: the path of the Compose file) --compatibility If set, Compose will attempt to convert keys in v3 files to their non-Swarm equivalent (DEPRECATED) --env-file PATH Specify an alternate environment fileCommands: build Build or rebuild services # 构建或者重构服务 config Validate and view the Compose file create Create services # 创建服务 down Stop and remove resources # 停止运行中的容器 events Receive real time events from containers exec Execute a command in a running container # 进入一个运行的容器中 help Get help on a command images List images # 列出存在的所有镜像 kill Kill containers # kill掉容器 logs View output from containers # 查看容器日志输出 pause Pause services # 暂停服务 port Print the public port for a port binding # 列出端口 ps List containers # 列出运行中的容器 pull Pull service images # 推镜像到仓库 push Push service images # 从仓库中拉取镜像 restart Restart services # 重启服务 rm Remove stopped containers # 删除停止的容器 run Run a one-off command scale Set number of containers for a service start Start services # 启动服务 stop Stop services # 停止服务 top Display the running processes # 展示运行容器使用的资源信息 unpause Unpause services up Create and start containers # 启动容器 version Show version information and quit # 查看docker-compose版本信息 2.2.3 安装harbor​ harbor默认使用https协议进行访问，也支持http协议。因此接下来的安装过程将分成两个步骤，使用http协议和https协议进行harbor安装。 2.2.3.1 解压并认识harbor配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# ls harbor-offline-installer-v2.2.1.tgz# tar zxvf harbor-offline-installer-v2.2.1.tgz # 解压harbor/harbor.v2.2.1.tar.gzharbor/prepareharbor/LICENSEharbor/install.shharbor/common.shharbor/harbor.yml.tmpl # 配置文件样本# cat harbor.yml.tmpl | grep -Ev &quot;^$|#&quot; # 简单认识配置模板文件hostname: reg.mydomain.com # 默认的域名，使用该域名需要域名解析，如果不做域名解析，可修改成harbor主机IP地址http: # http信息，默认端口信息都可以自行修改 port: 80 https: # https的端口和证书存放路径，默认使用Https协议，但使用https需要配置证书，若不使用可注释掉 port: 443 certificate: /your/certificate/path private_key: /your/private/key/pathharbor_admin_password: Harbor12345 # 登陆harbor仓库web界面默认密码，默认用户为adamin，建议修改database: # 数据库信息 password: root123 # 数据库root用户密码，建议修改 max_idle_conns: 50 max_open_conns: 1000data_volume: /data # 默认存储位置trivy: ignore_unfixed: false skip_update: false insecure: falsejobservice: max_job_workers: 10notification: webhook_job_max_retry: 10chart: absolute_url: disabledlog: level: info local: rotate_count: 50 rotate_size: 200M location: /var/log/harbor_version: 2.2.0proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy 2.2.3.2 使用http协议访问harbor​ harbor默认使用https协议，如果不想使用https协议，可以修改/etc/docker/daemon.json文件。 123456# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;http://hub-mirror.c.163.com&quot;], &quot;insecure-registries&quot;: [&quot;10.138.3.111:80&quot;]&#125;# 将harbor主机IP地址(如果配置了域名解析可直接使用域名)和端口写入该文件，让其受信任即可 ​ 注释掉harbor.yml.tmpl文件中关于https的内容 12345# vim harbor.yml.tmpl # 注释掉该文件中下面的内容https: port: 443 certificate: /your/certificate/path private_key: /your/private/key/path 2.2.3.3 使用https协议访问harbor​ 使用https协议需要证书，接下来我们使用cfssl进行自签证书。 安装cfssl 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64# 添加执行权限# chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64# 移动至环境变量路径mv cfssl_linux-amd64 /usr/local/bin/cfssl # 生成证书工具mv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo # 证书信息查看工具# 验证# cfssl versionVersion: 1.2.0Revision: devRuntime: go1.6# 生成ca证书配置文件# mkdir ssl# cd ssl &amp;&amp; cfssl print-defaults config &gt; ca-config.json # cat ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;168h&quot; # 过期时间，默认为168h，此处修改成87600h=10年 &#125;, &quot;profiles&quot;: &#123; &quot;www&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, # 默认是1年，这里修改成10年 &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot; ] &#125;, &quot;client&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot;, # 默认是1年，这里修改成10年 &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;# 生成ca证书信息文件# cfssl print-defaults csr &gt; ca-csr.json# cat ca-csr.json &#123; &quot;CN&quot;: &quot;example.net&quot;, # 标识具体的域 &quot;hosts&quot;: [ # 使用该证书的域名 &quot;example.net&quot;, &quot;www.example.net&quot; ], &quot;key&quot;: &#123; # 加密方式，一般使用rsa，大小使用2048 &quot;algo&quot;: &quot;ecdsa&quot;, # 加密方式，默认是ecdsa，此处使用rsa &quot;size&quot;: 256 # 默认是256，此处使用2048 &#125;, &quot;names&quot;: [ # 证书中包含的国家地区和城市信息 &#123; &quot;C&quot;: &quot;US&quot;, # 表示国家，修改成CN &quot;L&quot;: &quot;CA&quot;, # 区域，这里修改成SiChuan &quot;ST&quot;: &quot;San Francisco&quot; # 城市，修改成本地国内城市，ChengDu &#125; ]&#125;# 使用证书信息文件生成证书# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -2021/04/05 13:07:11 [INFO] generating a new CA key and certificate from CSR2021/04/05 13:07:11 [INFO] generate received request2021/04/05 13:07:11 [INFO] received CSR2021/04/05 13:07:11 [INFO] generating key: rsa-20482021/04/05 13:07:12 [INFO] encoded CSR2021/04/05 13:07:12 [INFO] signed certificate with serial number 697269161568410809548825101035635405041110584154# 执行完成后，将生成两个pem文件，如下# ls *.pemca-key.pem ca.pem# 签署服务端证书# cp ca-csr.json reg.com-csr.json # vim reg.com-csr.json&#123; &quot;CN&quot;: &quot;reg.com&quot;, # 跟harbor配置文件中的域名一致 &quot;hosts&quot;: [], &quot;key&quot;: &#123; # 加密方式 &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ # 证书中包含的国家地区和城市信息 &#123; &quot;C&quot;: &quot;CN&quot;, # 国家 &quot;L&quot;: &quot;SiChuan&quot;, # 区域 &quot;ST&quot;: &quot;ChengDu&quot; # 城市 &#125; ]&#125;# 签发证书# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www reg.com-csr.json | cfssljson -bare reg.com2021/04/05 13:07:56 [INFO] generate received request2021/04/05 13:07:56 [INFO] received CSR2021/04/05 13:07:56 [INFO] generating key: rsa-20482021/04/05 13:07:56 [INFO] encoded CSR2021/04/05 13:07:56 [INFO] signed certificate with serial number 1847975518655981104997457290808260420647146566582021/04/05 13:07:56 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).# ls *.pemca-key.pem ca.pem reg.com-key.pem reg.com.pem 修改harbor配置文件，将证书路径写入harbor配置文件中 123456789101112# ls /etc/docker/sslreg.com-key.pem reg.com.pem# vim harbor.yml.tmpl # 将路径写入即可···http port: 80 # 注释掉关于http的信息https: port: 443 certificate: /etc/docker/ssl/reg.com.pem private_key: /etc/docker/ssl/reg.com-key.pem··· 2.2.3.4 安装123456789101112131415161718192021# 准备配置文件# cd /root/harbor &amp;&amp; mv harbor.yml.tmpl harbor.yml # docker image load -i harbor.v2.2.1.tar.gz &amp;&amp; ./prepare # 安装并启动harbor容器# ./install.sh····Creating network &quot;harbor_harbor&quot; with the default driverCreating harbor-log ... doneCreating harbor-db ... doneCreating registry ... doneCreating harbor-portal ... doneCreating redis ... doneCreating registryctl ... doneCreating harbor-core ... doneCreating nginx ... doneCreating harbor-jobservice ... done✔ ----Harbor has been installed and started successfully.---- ## docker contianer ls 2.3 访问web页面​ 将reg.com域名解析写入windows主机中的hosts文件，做完域名解析后在web浏览器中输入域名，由于我们使用的是自签证书，因此还是会提示链接不安全(并显示在浏览器uri前面)，点击”添加另外”即可： 登陆界面如下，用户名admin，密码为harbor.yml文件中定义的admin密码： 进入控制台，不喜欢该主题的话，可以点击下方浅色主题进行切换 查看证书信息 证书信息： ​ docker仓库搭建就完成，docker仓库的使用方法，请参考下一个章节的文章。 第二章 harbor仓库的使用1.命令行登陆到harbor仓库12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 从另外的主机访问harbor仓库# docker login https://reg.comUsername: adminPassword: Error response from daemon: login attempt to https://reg.com/v2/ failed with status: 404 Not Found# 提示找不到https://reg.com，解决方法做hosts域名解析# echo &quot;10.138.3.111 reg.com&quot; &gt;&gt; /etc/hosts# 做完hosts域名解析后，再一次登陆，提示未知的认证# docker login https://reg.comUsername: adminPassword: Error response from daemon: Get https://reg.com/v2/: x509: certificate signed by unknown authority# 此时查看harbor主机上的messages日志，将看到访问失败的信息Apr 5 15:20:13 reg dockerd: time=&quot;2021-04-05T15:20:13.964537621+08:00&quot; level=error msg=&quot;Handler for POST /v1.41/auth returned error: login attempt to https://reg.com/v2/ failed with status: 404 Not Found&quot;Apr 5 15:24:35 reg dockerd: time=&quot;2021-04-05T15:24:35.416729925+08:00&quot; level=info msg=&quot;Error logging in to endpoint, trying next endpoint&quot; error=&quot;Get https://reg.com/v2/: x509: certificate signed by unknown authority&quot;# 解决方法，将harbor主机上的自签证书拷贝到需要访问harbor的主机上mkdir /etc/docker/certs.d/reg.com -p # 在被访问主机上新建目录，目录名称是/etc/docker/certs.d/harbor主机域名# 将harbor主机上的reg.com.pem文件拷贝到被访问主机上，名称后缀为.crt，不是.pem# scp -P 10022 /etc/docker/ssl/reg.com.pem 10.138.3.111:/etc/docker/certs.d/reg.com/reg.com.crt# 再次访问就没有问题了# docker login https://reg.comUsername: adminPassword: WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded# 查看主机上所有镜像# docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEphp lnmp_php_7.4.16 960a00c2710a 6 hours ago 712MBnginx lnmp_ningx_1.18.0 438f02d48ea9 15 hours ago 489MBmysql 5.7 cd0f0b1e283d 5 days ago 449MBcentos centos7.6.1810 f1cb7c7d58b7 2 years ago 202MB# 给centos镜像修改标签# docker tag centos:centos7.6.1810 reg.com/library/centos:centos7.6.1810# docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEphp lnmp_php_7.4.16 960a00c2710a 6 hours ago 712MBnginx lnmp_ningx_1.18.0 438f02d48ea9 15 hours ago 489MBmysql 5.7 cd0f0b1e283d 5 days ago 449MBcentos centos7.6.1810 f1cb7c7d58b7 2 years ago 202MBreg.com/library/centos centos7.6.1810 f1cb7c7d58b7 2 years ago 202MB# 推送centos镜像到reg.com仓库中# docker push reg.com/library/centos:centos7.6.1810The push refers to repository [reg.com/library/centos]89169d87dbe2: Pushed centos7.6.1810: digest: sha256:747b2de199b6197a26eb1a24d69740d25483995842b2d2f75824095e9d1d19eb size: 529 2.推送镜像到harbor仓库​ harbor仓库中给出的镜像推送方法提示： 镜像推送完成后，我们刷新页面，查看harbor页面上的镜像信息： 3. harbor集群 通过前文的方法，配置多台harbor主机，此处使用http协议进行访问，具体方法不再赘述。 安装完成后，登陆备库 登陆主库进行主备配置，点击仓库管理–&gt;新建目标，填入备库的访问地址，输入备库的访问用户和密码，如图所示： 测试连接： 1234567# 进入harbor安装目录，执行命令，停止harbor镜像仓库# docker-compose down # vim /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;,&quot;http://hub-mirror.c.163.com&quot;], &quot;insecure-registries&quot;: [&quot;10.138.3.222:80&quot;] # 将被控端域名添加信任&#125; 完成后，如下： 新建复制规则： 创建完成后，如下： 现在，我们推送一个镜像到reg.com主库中，查看备库10.138.3.222:80是否进行了同步 12345678910111213141516# docker tag mysql:5.7 reg.com/library/mysql:5.7# docker image ls | grep mysql mysql 5.7 cd0f0b1e283d 6 days ago 449MBreg.com/library/mysql 5.7 cd0f0b1e283d 6 days ago 449MB# docker login reg.com # 登陆主库Authenticating with existing credentials...WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded# 推送镜像到主库# docker push reg.com/library/mysql:5.7 查看reg.com主库，检查是否推送成功 查看reg.com上复制管理中的信息，显示复制成功 检查备库10.138.3.222:80是否成功同步了msyql:5.7 同步成功 配置主主 12345678# mkdir /etc/docker/certs.d/reg.com -p # 在备服务器上新建目录# 将reg.com主机上的认证文件拷贝到备服务器上# scp -P 10022 /etc/docker/ssl/reg.com.pem 10.138.3.222:/etc/docker/certs.d/reg.com/reg.com.crt# 查看备服务器上的认证文件# ls /etc/docker/certs.d/reg.com/reg.com.crt /etc/docker/certs.d/reg.com/reg.com.crt 主从复制+备份harbor数据目录/data/+数据库文件 12345678910111213141516171819202122# 日志文件# ls /var/log/harbor/core.log jobservice.log portal.log postgresql.log proxy.log redis.log registryctl.log registry.log# harbor数据文件ls /data/ca_download database job_logs redis registry secretls /data/registry/docker/registry/v2/repositories/library/centos/_layers _manifests _uploads # library/centos是我们刚刚上传的centos镜像# ls /data/secret/cert core keys registry# ls /data/secret/core/private_key.pem# ls /data/secret/cert/server.crt server.key# ls /data/secret/keys/secretkey# ls /data/secret/registry/root.crt 附：忘记harbor登陆密码的解决方法123456789101112131415161718192021222324252627282930313233# echo -n &quot;Admin@2020&quot;|md5sum06d1108043ad13aebdca098c7e6bfe64 -[root@reg ~]# docker exec -it harbor-db /bin/bash # 进入数据库容器 postgres [ / ]$ netstat -lntp Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:5432 0.0.0.0:* LISTEN 1/postgres tcp 0 0 127.0.0.11:34651 0.0.0.0:* LISTEN - tcp6 0 0 :::5432 :::* LISTEN 1/postgres postgres [ / ]$ psql -U postgres -d postgres -h 127.0.0.1 -p 5432 # 登陆数据库psql (9.6.21)Type &quot;help&quot; for help.postgres=# helpYou are using psql, the command-line interface to PostgreSQL.Type: \\copyright for distribution terms \\h for help with SQL commands \\? for help with psql commands \\g or terminate with semicolon to execute query \\q to quitpostgres=# \\c registry # 切换到registry数据库中You are now connected to database &quot;registry&quot; as user &quot;postgres&quot;.registry=# select * from harbor_user; user_id | username | password |realname | salt |---------+-----------+---------------------------------+---------------+-----------------------------+ 2 | anonymous | | anonymous user| | 1 | admin |b5361d73851310e3f99e417c21f00fff | system admin |F9xavf065XBc44obZewfrSrVkUoF42Gd (2 rows) 第三章 prometheus+grafana监控docker主机监控系统概述： cAdvisor(container cAdvisor)：用于收集正在运行的容器资源使用和性能信息，在每个docker 主机上部署 https://github.com/google/cAdvisor Prometheus：容器监控系统，从cAdvisor收集并存储数据，官网https://prometheus.io/ Grafana：开源可视化展示系统，官网https://grafana.com/grafana 1.配置cAdvisor1.1 安装cAdvisor​ 可通过GitHub中cAdvisor的运行容器命令，运行cAdvisor 1234567891011121314151617181920212223242526272829303132docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:ro \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ --privileged \\ --device=/dev/kmsg \\ google/cadvisor:latest # Github中的源地址可能访问不了，因此这里更换成dockerhub中的地址Unable to find image &#x27;google/cadvisor:latest&#x27; locallylatest: Pulling from google/cadvisorff3a5c916c92: Downloading 44a45bb65cdf: Download complete 0bbe1a2fe2a6: Download complete latest: Pulling from google/cadvisorff3a5c916c92: Pull complete 44a45bb65cdf: Pull complete 0bbe1a2fe2a6: Pull complete Digest: sha256:815386ebbe9a3490f38785ab11bda34ec8dacf4634af77b8912832d4f85dca04Status: Downloaded newer image for google/cadvisor:latestWARNING: IPv4 forwarding is disabled. Networking will not work. # 此处有一个报错，需要打开路由转发功能e1132c19a4129804a28c1c0d0d61884ced3d548625daf882834095a420d3da7a# docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES886e2bce936f google/cadvisor:latest &quot;/usr/bin/cadvisor -…&quot; 12 seconds ago Up 11 seconds 0.0.0.0:8080-&gt;8080/tcp cadvisor60d4d688b698 php:lnmp_php_7.4.16 &quot;./sbin/php-fpm -c /…&quot; 6 hours ago Restarting (78) 3 seconds ago lnmp_php10124a1d6b8b mysql:5.7 &quot;docker-entrypoint.s…&quot; 16 hours ago Up 7 hours 33060/tcp, 0.0.0.0:13306-&gt;3306/tcp lnmp_mysql 1.2 访问 cAdvisor​ 直接在浏览器中输入url–http://10.138.3.130:8080访问cAdvisor，访问较慢 第一次实验的时候一直不能通过http://10.138.3.130:8080/访问，重新删除并安装cAdvisor容器，发现执行完新建cAdvisor容器后有一个报错信息`WARNING: IPv4 forwarding is disabled. Networking will not work.，解决方法是打开路由转发功能echo ‘net.ipv4.ip_forward=1’ &gt;&gt; /usr/lib/sysctl.d/00-system.conf` 查看宿主机上运行的容器信息： 2. 配置prometheus2.1 安装prometheus​ prometheus可部署在任意的宿主机上， cAdvisor暴露出来的指标必须要符合prometheus的格式，才能被prometheus采集，我们在浏览器中输入http://10.138.3.130:8080/metrics，即可查看到符合prometheus格式的信息 12345# 安装prometheus# docker run -d --name prometheus -p 9090:9090 prom/prometheus# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfe3468745563 prom/prometheus &quot;/bin/prometheus --c…&quot; 6 seconds ago Up 5 seconds 0.0.0.0:9090-&gt;9090/tcp prometheus 2.2 访问prometheus​ 在浏览器中访问http://ip:9090/config，如下： 2.3 添加监控主机​ 修改prometheus配置文件，让prometheus采集数据，每一个被监控端的信息都需要写入prometheus的配置文件，prometheus才能采集到数据，更改配置文件就是让prometheus知道去哪里取数据。 12345678910111213141516171819# 进入prometheus容器中# docker exec -it fe3468745563 sh /prometheus $ vi /etc/prometheus/prometheus.yml scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: &#x27;prometheus&#x27; # 等同于分组的组名 static_configs: - targets: [&#x27;localhost:9090&#x27;] # 被监控的目标端 - job_name: &#x27;docker&#x27; # 新建一个docker分组 static_configs: - targets: [&#x27;10.138.3.130:8080&#x27;] # 添加cAdvisor的接口，就能监控到数据；多个被监控端，用逗号隔开# 重启生效# docker container restart fe3468745563 # docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfe3468745563 prom/prometheus &quot;/bin/prometheus --c…&quot; 17 minutes ago Up 14 seconds 0.0.0.0:9090-&gt;9090/tcp prometheus ​ 此时就能在prometheus的targets页面查看新添加的主机信息： ​ 现在我们可以通过在prometheus的Graph页面输入promql查看指定的性能数据了 ​ 如下，这里只要输入一个字符，就能列出跟含有该字符的所有的promql，非常方便 3.配置Grafana3.1 安装Grafana1234# docker run -d --name grafana -p 3000:3000 grafana/grafana# docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES99ec838588e1 grafana/grafana &quot;/run.sh&quot; 39 seconds ago Up 37 seconds 0.0.0.0:3000-&gt;3000/tcp grafana 3.2 登陆Grafana​ 登陆grafana，用户和密码都是admin，登陆后会要求修改密码。 ​ 修改密码后(Admin)，进入主页： 3.3 配置Grafana3.3.1 添加数据源–Prometheus​ 首先，添加一个数据源–即告诉Grafana去哪里获取数据，点击DATA SOURCES，使grafana连接到prometheus，下图页面展示了所有能添加的数据源。 ​ 输入Prometheus的访问地址，并保存。 ​ 查看添加的数据源信息： 3.3.2 添加自定义仪表盘 ​ 默认的仪表盘页面(更正：下文中截图Prometheus的查询语句promql，误写成了proql)如下： ​ 此时我们即将使用到promql，即将promql添加到Grafana仪表盘中的Metrics中Grafana将在该图形中展示哪种promethues中的数据，我们依旧可以通过prometheus界面上的Graph进行调试，如监控内存，我们列出所有跟容器相关的container_memory_usage_bytes的信息 ​ 下面，我们通过筛选条件，列出容器的container_memory_usage_bytes信息，promql语句为container_memory_usage_bytes&#123;image!=&quot;&quot;&#125; ​ 接下来，我们采集具体的容器，如名称为cAdvisor的容器的信息，promql为container_memory_usage_bytes&#123;image!=&quot;&quot;,name=&quot;cadvisor&quot;&#125;，如下 ​ 将上面得到的promql写入到grafana中的metrics，就能得到数据 ​ 完成后，如下图所示： 3.3.3 添加模板仪表盘​ 如果每台机器都要这样手动配置一个图形的话，对于十来台服务器来说还可以接受，但对于成有千上万台的服务器的公司来说，这样的配置方式无疑是灾难性的，因此有没有一个通用的模板呢？答案是肯定的，grafana官网提供了很多这样的监控模板，访问地址–https://grafana.com/grafana/dashboards/： ​ 监控docker主机模板ID：193，即使用url–https://grafana.com/grafana/dashboards/193 在grafana首页中，点击manage–&gt;import，输入193导入模板 输入193点击load按钮 点击load后，自动加载 导入后，即可看到仪表盘数据内容 ​ 这里的仪表盘只是大佬给我们做好了的模板，本质上也是通过promql自定义仪表盘来做的，例如，我们查看上图中的Total Memory Usage，点击Edit来到编辑界面 ​ promql语句如下： 一些常见的坑： 不展示数据： 可能是宿主机由于时间不同步 promql格式有问题，可以通过prometheus的Graph进行检查 prometheus本来就没有数据 cAdvisor+prometheus+grafana进行监控的步骤总结： 在被监控主机上安装cAdvisor容器，让cAdvisor收集宿主机数据 将宿主机监控端写入prometheus配置文件，让Prometheus能够获得cAdvisor采集到宿主机的监控数据 添加到grafana仪表盘(但是每天主机都添加一个仪表盘是不是很烦呢？能否一个界面显示所有的主机呢？需要查看哪台主机就点击哪台主机呢？) ​ 点击下图中的设置按钮，进入设置选项 ​ 点击Versions，添加变量 ​ 如下，列出所有分组(或主机)信息，即dashboard中是按照job还是instance显示主机： ​ 上图中的配置，我们仍然是通过promql实现，在Graph页面显示不出的数据，不可能在grafana中能显示图形。 ​ 此时，查看grafana仪表盘，就多了选择框，可以选择主机或分组，上图中add variables(添加变量)时使用 的是instance就显示所有主机的信息，添加的是job就会显示所有分组的信息。 ​ 但是，现在我们选择两个不同的节点，图表的结果是一样的，因为promql中的语句写死了，没有采集到对应node节点主机的信息，因此需要修改，方法如下，先点击进入其中一个仪表盘的修改界面： ​ 添加上每台主机，在原promql的基础上，添加上&quot;instance=&#39;$Node&#39;&quot;，变量Node是我们上文中通过add variables自定义的变量名称 ​ 通过这样的修改，得到的就是每一台主机的内存使用量了，然后再将该仪表盘中的其他仪表盘promql都进行这样的修改，就能展示出每台主机独特的信息。","categories":[{"name":"docker","slug":"docker","permalink":"https://scaven.site/categories/docker/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"https://scaven.site/tags/kubernets/"}]}],"categories":[{"name":"docker","slug":"docker","permalink":"https://scaven.site/categories/docker/"},{"name":"Home","slug":"Home","permalink":"https://scaven.site/categories/Home/"}],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"https://scaven.site/tags/kubernets/"},{"name":"AboutMe","slug":"AboutMe","permalink":"https://scaven.site/tags/AboutMe/"},{"name":"welcome","slug":"welcome","permalink":"https://scaven.site/tags/welcome/"}]}